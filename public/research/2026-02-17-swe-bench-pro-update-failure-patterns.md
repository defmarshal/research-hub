# SWE-Bench Pro 2026-02 Update: Leaderboard Surge and Persistent Failure Modes

**Priority**: ðŸ”´ HIGH (brownfield failure patterns â€” enterprise ROI impact)

---

## Executive Summary

Fresh data from Scale AI's SWEâ€‘Bench Pro public leaderboard (last updated 2026â€‘01â€‘16) reveals **significant improvements** in top model performance while **barely moving the needle** on private codebases. The gap between slick demo capabilities and realâ€‘world software engineering tasks remains starkâ€”enterprise planners must adjust ROI models accordingly.

**Key numbers (Public dataset, 731 tasks):**
- **Claudeâ€‘Opusâ€‘4.5** (2025â€‘11â€‘01): **45.89%** resolve rate (â†‘ from ~23% in SepÂ 2025)
- **Claudeâ€‘4.5â€‘Sonnet**: 43.60%
- **Geminiâ€‘3â€‘Proâ€‘preview**: 43.30%
- **GPTâ€‘5 (High)**: 41.78%
- **Qwen3â€‘Coderâ€‘480B**: 38.70%

**Private subset (276 tasks, proprietary codebases):**
- **Claudeâ€‘Opusâ€‘4.1**: 17.8% (down from 22.7% public)
- **GPTâ€‘5**: 14.9% (down from 23.1% public)

**Implication**: Public leaderboard overstates enterprise readiness by **2â€“3Ã—**. Realâ€‘world brownfield environments cut success rates by >60% for top models.

---

## 1. Benchmark Methodology & Scope

SWEâ€‘Bench Pro is designed to evaluate AI agents on **longâ€‘horizon, realistic software engineering tasks**:

- **Dataset size**: 1,865 tasks across 41 professional repositories
- **Task complexity**: average solution = 107 lines of code across 4.1 files
- **Sources**: Consumer apps, B2B platforms, developer tools (both openâ€‘source with strong copyleft licenses and private proprietary codebases)
- **Scoring**: Resolve rate requires (a) fixing the specific issue (failâ€‘toâ€‘pass tests) AND (b) no regressions (passâ€‘toâ€‘pass tests still pass)
- **Evaluation**: Humanâ€‘augmented problem statements, reproducible Docker environments, contaminationâ€‘resistant by design

This is the **most rigorous** realâ€‘world coding benchmark available as of 2026â€‘02.

---

## 2. Performance Landscape (Jan 2026)

### 2.1 Public Leaderboard

Top 5 models achieve **40â€“46%** resolve ratesâ€”substantial gain from the ~23% plateau observed in midâ€‘2025, but still meaning **more than half of tasks fail**.

| Rank | Model | Resolve Rate (Public) | Notes |
|------|-------|----------------------|-------|
| 1 | Claudeâ€‘Opusâ€‘4.5 | 45.89% Â± 3.60 | New leader; thinking model |
| 2 | Claudeâ€‘4.5â€‘Sonnet | 43.60% Â± 3.60 | Strong sibling |
| 3 | Geminiâ€‘3â€‘Proâ€‘preview | 43.30% Â± 3.60 | Close behind |
| 4 | Claudeâ€‘4â€‘Sonnet | 42.70% Â± 3.59 | Consistent |
| 5 | GPTâ€‘5 (High) | 41.78% Â± 3.49 | Slightly behind top Claude |

Midâ€‘tier:
- Qwen3â€‘Coderâ€‘480B: 38.70%
- Minimaxâ€‘2.1: 36.81%
- GPTâ€‘5.2â€‘Codex: 41.04%
- Kimiâ€‘K2â€‘Instruct: 27.67%

Smaller models (<100B) mostly <20%: Llama4â€‘Maverick (5.24%), Codestral (1.51%).

**Cost note**: The top models were run with **uncapped cost** and **turn limit 250**. Realâ€‘world deployments may impose tighter budgets, potentially reducing effective performance.

### 2.2 Private Subset Collapse

When evaluated on **private, unseen proprietary codebases** (18 startups), performance **plummets**:

- Claudeâ€‘Opusâ€‘4.1: **17.8%** (vs 22.7% public)
- GPTâ€‘5: **14.9%** (vs 23.1% public)

The ~40% relative drop suggests **severe generalization gaps**. Private codebases likely have:
- Less documentation
- More technical debt
- Proprietary frameworks/patterns
- Less test coverage
- Different coding styles

These are precisely the **brownfield environments** enterprises care about.

---

## 3. Failure Mode Analysis (What's Still Broken)

Even with top models at ~45% on public tasks, **55%+ failure rate** indicates fundamental limitations. The leaderboard highlights these patterns:

### 3.1 Repositoryâ€‘Specific Difficulty

- **Some repositories prove consistently difficult for *all* models**, with resolve rates below 10%.
- **Others are tractable**, with certain models achieving >50%.
- **Drivers**: codebase complexity, problem type, documentation quality, test setup.

**Enterprise implication**: ROI depends heavily on **which codebases** you target. Legacy monoliths, custom frameworks, and poorly tested repos will see far lower success rates than the headline numbers.

### 3.2 Language Gaps

Performance varies dramatically by programming language:

- **Go & Python**: some models exceed 30% resolve
- **JavaScript / TypeScript**: more variable, often lower (some models near 0% on JS/TS tasks)

This suggests **languageâ€‘specific weaknesses** in training data or toolchain integration. Enterprises with heavy JS/TS stacks should be wary.

### 3.3 Scalability of Changes

Model performance **degrades significantly** as solution complexity increases:
- More lines of code required â†’ lower success
- More files to edit â†’ lower success
- Reference solutions average **107 lines across 4.1 files** â€” far beyond trivial oneâ€‘file patches

Most realâ€‘world bug fixes or features are *larger* than this average, implying **realâ€‘world success rates may be even lower**.

### 3.4 Private vs Public Gap

The **60â€“65% drop** on private codebases is the most critical finding. Models that excel on openâ€‘source projects (with active issues, good docs, community tests) **collapse** on internal enterprise code.

Why?
- Different architectural patterns
- Proprietary libraries with no public equivalents
- Lack of available context (internal wikis, design docs not in training data)
- Security constraints limiting agent actions

---

## 4. Comparison with Earlier Assessments

Our earlier brownfield failure patterns report (2026â€‘02â€‘16) cited **~23%** resolve rates. The new leaderboard shows top models at **~46%** on public tasks â€” indicating **rapid improvements** in the last 3â€“4 months.

However:
- Private subset remains **stubbornly low** (15â€“18%)
- The publicâ€“private gap has **not narrowed**
- Absolute numbers still mean **>50% failure** on even the easiest public repos

Thus, the **core thesis holds**: enterprises must categorize codebases by difficulty and apply targeted mitigations. But the **thresholds have shifted**: what was "easy" (public) now sees nearâ€‘50% success, while "hard" (private) remains <20%.

---

## 5. Enterprise ROI Implications

### 5.1 Adjusting Expectations

- **Do not use public leaderboard numbers** to justify enterprise spend. They overstate readiness by 2â€“3Ã—.
- **Expect 15â€“25% success** on internal, proprietary codebases with top models (Claude, GPTâ€‘5, Gemini).
- **Factor human review**: For every 4 tasks the AI attempts, a human will likely need to fix or complete ~3â€“4 of them.

### 5.2 Costâ€‘Benefit Reâ€‘calibration

If AI agent costs per task are nonâ€‘trivial, the **effective cost per *successfully* resolved issue** becomes:

```
Effective cost = (Agent cost per attempt) / (Success rate)
```

Example: Agent costs $0.50 per attempt, 20% success â†’ effective cost = $2.50 per resolved issue. This must be compared against **fully manual** or **hybrid** workflows.

### 5.3 Triage by Codebase Type

Differentiate:
- **Greenfield projects** (new code, modern stacks): higher AI success (~40â€“50%)
- **Mature openâ€‘source components** (wellâ€‘tested, documented): moderate success (~30â€“40%)
- **Legacy brownfield** (old frameworks, low test coverage): low success (<20%)
- **Proprietary internal stacks** (custom frameworks, limited docs): very low (<15%)

Deploy AI agents **only on the tractable subset** initially; expand as models improve.

---

## 6. Recommendations for Buyers & Developers

1. **Run a pilot on *your own code*** â€” not on synthetic benchmarks. Use a representative subset of internal repos.
2. **Measure success as "no regressions + issue fixed"** â€” not just code that compiles.
3. **Track failure modes** internally: Are agents failing on:
   - Ambiguous requirements? â†’ improve prompting/context
   - Multiâ€‘file coordination? â†’ better planning tools
   - Legacy APIs? â†’ provide more documentation
   - Test environment setup? â†’ improve scaffolding
4. **Budget for human oversight** â€” plan on 2â€“4Ã— attempts per successful resolution.
5. **Negotiate pricing based on *effective* success**, not raw API calls.

---

## 7. Gaps & Next Steps

- **Need longitudinal data**: Are success rates improving monthâ€‘overâ€‘month, or is this a oneâ€‘time bump?
- **Missing failure taxonomy**: The leaderboard shows *that* tasks fail, not *why*. We need detailed postâ€‘mortems (see immediate leads below).
- **Tool augmentation impact**: How much does adding web search, code search, or internal documentation retrieval improve success? Not measured here.
- **Costâ€‘normalized performance**: With uncapped cost, top models used extensive compute. What is the performance plateau under budget constraints?

---

## 8. Immediate Research Leads

1. **SWEâ€‘Bench Pro failure log mining** â€” The public dataset includes trajectories. Scrape failure patterns: where do agents diverge from reference solutions? (early exit, wrong file, test error, etc.)
2. **Private subset deep dive** â€” Acquire or infer details about the 18 startup codebases: languages, frameworks, size. Identify what makes them harder.
3. **Enterprise case studies** â€” Interviews with companies that piloted AI coding agents on internal repos; gather their observed success rates and failure categories.
4. **Toolâ€‘augmented variants** â€” Evaluate agents with retrieval (RAG) over internal docs; compare to bare scaffold results.

---

## 9. Conclusion

SWEâ€‘Bench Pro remains the gold standard for evaluating AI software engineers. As of JanÂ 2026, **top models achieve ~45% on public tasks** â€” a notable improvement, but still far from reliable deployment. The **private subset failure rate (~85%) is a red flag** for enterprise buyers.

Bottom line: **Brownfield coding is still largely a research problem**. Budget for substantial human augmentation; focus pilots on tractable codebases; monitor monthly leaderboard updates for acceleration.

---

_Sources: Scale AI SWEâ€‘Bench Pro public leaderboard (accessed 2026â€‘02â€‘17), SWEâ€‘Bench Pro paper (https://scale.com/research/swe_bench_pro), GitHub repository (https://github.com/scaleapi/SWE-bench_Pro-os)_

_Report compiled: 2026â€‘02â€‘17Â UTC_

_Gap priority: HIGH (brownfield failure patterns â€” updated with fresh data)_
