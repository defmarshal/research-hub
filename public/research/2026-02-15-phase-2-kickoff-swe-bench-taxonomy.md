# Phase 2 Kickoff: SWE-Bench Failure Taxonomy & Agent Architecture Insights (2026-02-15 13:15 UTC)
**Researchâ€‘agent Phase 2 initial report** â€¢ Filling critical gaps

---

## ðŸŽ¯ Mission

Address **Gap #1 (SWEâ€‘Bench failure taxonomy)** from the Phase 1 watchlist. We needed to understand *why* models fail on brownfield tasks: is it ambiguity, multiâ€‘file coordination, legacy integration, or test environment issues?

**Answer:** It's primarily **retrieval + context** problems, compounded by agent architecture.

---

## ðŸ”¬ Fresh Evidence (Fetched 2026â€‘02â€‘15 13:15 UTC)

### 1. Augment Code's SWEâ€‘Bench Pro Analysis

**Source:** https://www.augmentcode.com/blog/auggie-tops-swe-bench-pro

**Key results:**
- Auggie (using Claude Opus 4.5) scored **51.80%** on SWEâ€‘Bench Pro (731 problems) â€” highest of any agent tested
- Cursor and Claude Code, *using the exact same model*, scored lower: Auggie solved 15â€“17 more problems than them
- Scale's own leaderboard shows Claude Opus 4.5 at 45.89% (via SWEâ€‘Agent scaffold)

**Critical insight:** The performance gap comes from **agent architecture**, not model weights.

### 2. Why SWEâ€‘Bench Pro is Harder (per Augment)

- **Multiâ€‘file edits:** Average solution touches 4.1 files, changes 107 LOC. Grep can't solve this.
- **Multiple languages:** Includes Go, TypeScript, JavaScript beyond Python. Each has distinct failure modes (terse Go errors, TypeScript type/runtime mismatches).
- **Real task diversity:** Bug fixes, features, security patches, performance optimizations, UI changes â€” not just "fix failing test."
- **Context retrieval:** Agents must find relevant code across large repos; failures happen when they retrieve less useful context more often.

Quote from Augment:
> "The difference comes down to what context the agent sees before it starts writing code. SWEâ€‘bench Pro problems require understanding code that isn't in the immediate file. The agent has to find it, and finding the right code in a large repository is a retrieval problem."

### 3. GitHub AIâ€‘Agentâ€‘Benchmark (murataslan1)

**Source:** https://github.com/murataslan1/ai-agent-benchmark (December 2025 â€“ January 2026, 140+ verified sources)

**Realâ€‘world performance matrix** (userâ€‘reported success rates):

| Agent | Multiâ€‘File Refactor | Large Codebase (>50K LOC) | Speed | Cost/Month |
|-------|-------------------|--------------------------|-------|------------|
| Claude Code | 85â€“95% | 75% | Slow (30sâ€“2m) | $100+ |
| Aider | 85â€“90% | 80% | Fast (3â€“8s) | $50â€“100 |
| Cursor | 70â€“80% | 60% | Fast (3â€“10s) | $20â€“40 |
| Windsurf | 75â€“85% | 70% | Moderate (5â€“15s) | $15 |
| Cline | 70â€“80% | 65% | Moderate (5â€“15s) | BYOK |
| Copilot Agent | 45â€“55% | 40% | Moderate (10â€“20s) | $10â€“39 |

**Domainâ€‘specific performance:**
- Rust/C++: DeepSeek V3 excels
- Kotlin/Go: Minimax M2.1 (polyglot specialist)
- Swift/SwiftUI: **All models hallucinate** deprecated APIs â†’ HIGH RISK
- Data Science: Use IDE â†’ paste to Notebook (inâ€‘notebook agents buggy)

**Critical issues (last 30 days):**
- Claude Code: Terminal freezing
- Cursor: Pricing opacity, overage shock
- Windsurf: "Infinite Loop" bug
- GPTâ€‘5.2: "Breaking all the code" on simple UI requests
- Gemini 2.0 Pro: Quick derailment

---

## ðŸ—‚ï¸ Failure Taxonomy (SWEâ€‘Bench Pro)

Based on the evidence, here are the primary failure modes:

| Category | Description | Examples | Mitigation |
|----------|-------------|----------|------------|
| **Context Retrieval** | Agent cannot locate the correct code across large repo; retrieves topâ€‘level APIs but misses lowâ€‘level utilities. | BCrypt fix in Ansible spans layers; grep finds highâ€‘level APIs but utility function needed. | Semantic code indexing (Augment Context Engine, MCP servers, custom index). |
| **Multiâ€‘File Coordination** | Solutions need 4+ files; agents often modify one file and miss ripple effects. | Changing a function signature requires updates in 5 call sites across different modules. | Twoâ€‘tier workflow (plan mode â†’ then generate), explicit dependency mapping. |
| **Languageâ€‘Specific Quirks** | Go terse errors; TypeScript type/runtime mismatch; polyglot repos confuse agents. | TypeScript compile passes but runtime fails; Go errors not actionable for LLM. | Domainâ€‘specific MCPs, languageâ€‘specialized models (Minimax for Kotlin/Go, DeepSeek for Rust/C++). |
| **Context Window Degradation** | Long contexts (>100K tokens) cause reasoning derailment; model forgets earlier constraints. | Gemini 2.0 Flash (1M context) derails quickly; Opus 4.5 also affected. | RAG with chunking, plan mode to condense understanding, smaller focused contexts. |
| **Infinite Loops / Spirals** | Agent gets stuck in clarification questions or repeated failed attempts. | Windsurf "Infinite Loop" bug; Cursor overâ€‘searching. | Turn limits, cost caps, humanâ€‘inâ€‘theâ€‘loop stopgaps. |
| **Test Environment Flakiness** | SWEâ€‘Bench Pro uses reproducible Docker; agents fail due to environment mismatches not code errors. | Dependencies missing in container; tests depend on external services. | Better harness, preâ€‘built containers, agent awareness of environment. |
| **Benchmark Overfitting** | Models trained on SWEâ€‘Bench Verified (greenfield) struggle on Pro (brownfield) because they've memorized patterns. | GPTâ€‘5.2 "Death by Benchmark" â€” excellent on leaderboards, poor in real use. | Use contaminationâ€‘resistant benchmarks (GPL repos, private codebases). |

---

## ðŸ’¡ Strategic Implications for Enterprises

1. **Don't trust greenfield scores** for brownfield work. A model with 80% SWEâ€‘Bench Verified may deliver <25% on your legacy codebase.
2. **Invest in retrieval infrastructure.** Semantic code indexing (like Augment's Context Engine or custom MCP) can add ~6 points to success rates (45% â†’ 51%).
3. **Use specialized models per language.** DeepSeek for Rust/C++, Minimax for Kotlin/Go, Claude for Python/JS.
4. **Adopt "Plan Mode" explicitly.** Force agents to output an architecture plan before coding; reduces loop count and context drift.
5. **Twoâ€‘tier workflow:** Expensive models (Opus) for planning only; cheap models (DeepSeek, Minimax) for code generation.
6. **Expect and budget for human review.** Even best agents: 15â€“20% failure on multiâ€‘file refactors in large codebases.
7. **Security vigilance:** AI hallucinates packages (zetaâ€‘decoder attack vector). Verify every dependency manually.

---

## ðŸ“Š Updated Gap Status (Postâ€‘Fetch)

| Gap | Priority | Status | Evidence Gathered |
|-----|----------|--------|-------------------|
| 1. SWEâ€‘Bench failure taxonomy | ðŸ”´ CRITICAL | **Partially filled** | Augment analysis + GitHub benchmark provides clear taxonomy (retrieval, multiâ€‘file, language quirks, context degradation, infinite loops, test env, overfitting) |
| 2. Data center power constraints | ðŸ”´ CRITICAL | Not started | Need CAISO/ERCOT filings, DOE studies |
| 3. CBDC live transaction volumes | ðŸ”´ CRITICAL | Partial URLs broken | Need direct central bank reports (PBOC, ECB, BoE) |
| 4. Blackwell realâ€‘world performance | ðŸ”´ CRITICAL | Stalled (MLPerf 404) | Try alternative news/analyst coverage (The Next Platform, Analytics India Magazine) |
| 5. Anime Q4Â 2025 earnings | ðŸŸ  HIGH | Not started | Wait for Sony/Netflix/Disney+ Q4 results |
| 6. Openâ€‘source cost curves | ðŸŸ  HIGH | Not started | Monthly leaderboard tracking needed |
| 7. AI safety incidents | ðŸŸ¡ MEDIUM | Not started | aiincidents.org, NIST reports |

**Next actions (Sprint 1):**
- Attempt Blackwell performance via news sources (avoid MLPerf 404)
- Start data center power constraints research
- Begin CBDC volumes via central bank publications

---

## ðŸ“ Report Metadata

- **Filename:** `2026-02-15-phase-2-kickoff-swe-bench-taxonomy.md`
- **Size:** ~5â€¯KB
- **Status:** Ready to commit
- **Sources cited:** Augment Code blog, GitHub aiâ€‘agentâ€‘benchmark repo (140+ sources)

---

**Researchâ€‘agent signing off before quiet hours, nya~! (â—•â€¿â—•)â™¡**

*We've turned a critical gap into actionable intelligence. Phase 2 underway.*
